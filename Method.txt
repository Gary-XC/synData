ğŸš¦ Workflow per dataset/table

Bring in new dataset (e.g. customers.csv)

Preprocess it with TabularPreprocessor (learn scaling + one-hot layout).

Train a GAN generator + discriminator until results are good.

Save artifacts once

preproc.joblib â†’ remembers how to transform raw rows into the GANâ€™s feature space.

gan_best.pth â†’ generator weights matched to that exact feature count.

Reuse forever (as long as schema is stable)

Load gan_best.pth + preproc.joblib.

Sample as many synthetic rows as you need (daily, hourly, etc.).

No retraining required.

Retrain only when necessary

Schema changes (new columns, new categories, or dropped fields).

Data drift: distributions in the real data shift too far from what the model was trained on.

Scheduled refresh (e.g. weekly/monthly) if you want models to always reflect the most recent reality.

ğŸ”’ Why one model per dataset/schema

The generatorâ€™s output dimension is fixed to the number of processed features.

If the dataset has a different schema (32 vs 45 vs 100 features), you need a different generator trained for that.

So yes: each new dataset/table (with its own schema) gets trained once, saved, and reused.

ğŸ› ï¸ In practice

Customers â†’ train once â†’ outputs/customers/ has gan_best.pth + preproc.joblib.

Accounts â†’ train once â†’ outputs/accounts/â€¦

Transactions â†’ train once â†’ outputs/transactions/â€¦

Then your daily synthetic job just picks the right folder, loads that pair of files, and generates/upload rows â€” no retraining.