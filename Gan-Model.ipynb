{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264065d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Data: https://www.kaggle.com/datasets/architsharma01/loan-approval-prediction-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704552a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "FILE_PATH   = \"Loan_default.csv\"\n",
    "BATCH_SIZE  = 32\n",
    "LATENT_DIM  = 64          # noise vector size for the generator\n",
    "TARGET_COL  = None        # e.g., \"defaulted\" for supervised; None for GAN-style (label == input)\n",
    "DEVICE      = \"cpu\"\n",
    "PRETRAINED  = False       # set True to load from PRETRAIN_PATH\n",
    "PRETRAIN_PATH = \"pretrained_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a single generation block function\n",
    "def FC_Layer_blockGen(input_dim, output_dim):\n",
    "    single_block = nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return single_block\n",
    "\n",
    "# DEFINING THE GENERATOR\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# defining a single discriminator block       \n",
    "def FC_Layer_BlockDisc(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4)\n",
    "    )\n",
    "\n",
    "# Defining the discriminator\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "#Defining training parameters\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "lr = 0.0002\n",
    "num_features = 6\n",
    "latent_dim = 20\n",
    "\n",
    "# MODEL INITIALIZATION\n",
    "generator = Generator(latent_dim, num_features)\n",
    "discriminator = Discriminator(num_features)\n",
    "\n",
    "# LOSS FUNCTION AND OPTIMIZERS\n",
    "criterion = nn.BCELoss()\n",
    "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86432f8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "file_path = 'Loan_default.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "# X = data.values\n",
    "# X_normalized = torch.FloatTensor((X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) * 2 - 1)\n",
    "# real_data = X_normalized\n",
    "\n",
    "#Creating a dataset\n",
    "\n",
    "class AutoTabularDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Automatically preprocesses tabular data with mixed numeric + categorical features.\n",
    "      - Numeric: median impute -> MinMaxScaler(feature_range=(-1, 1))\n",
    "      - Categorical: most-frequent impute -> OneHotEncoder(handle_unknown='ignore'), dense (0/1)\n",
    "    Returns tensors suitable for PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe: pd.DataFrame,\n",
    "        target: str | None = None,\n",
    "        preprocessor: ColumnTransformer | None = None,\n",
    "        fit_preprocessor: bool = True,\n",
    "        device: torch.device | str | None = None,\n",
    "    ):\n",
    "        df = dataframe.copy()\n",
    "\n",
    "        # Separate X / y\n",
    "        if target is not None:\n",
    "            if target not in df.columns:\n",
    "                raise ValueError(f\"target='{target}' not found in dataframe columns.\")\n",
    "            y_raw = df[target]\n",
    "            X_raw = df.drop(columns=[target])\n",
    "        else:\n",
    "            # For GAN-like usage: label == input\n",
    "            X_raw = df\n",
    "            y_raw = None\n",
    "\n",
    "        # Identify column types\n",
    "        numeric_cols = X_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_cols = X_raw.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "        # Pipelines\n",
    "        numeric_pipeline = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", MinMaxScaler(feature_range=(-1, 1))),\n",
    "        ])\n",
    "\n",
    "        # Use dense (non-sparse) output for easy torch conversion.\n",
    "        try:\n",
    "            categorical_pipeline = Pipeline(steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "            ])\n",
    "        except TypeError:\n",
    "            # For older scikit-learn versions that use 'sparse' instead of 'sparse_output'\n",
    "            categorical_pipeline = Pipeline(steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "            ])\n",
    "\n",
    "        # ColumnTransformer\n",
    "        if preprocessor is None:\n",
    "            self.preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num\", numeric_pipeline, numeric_cols),\n",
    "                    (\"cat\", categorical_pipeline, categorical_cols),\n",
    "                ],\n",
    "                remainder=\"drop\",\n",
    "                verbose_feature_names_out=False,\n",
    "            )\n",
    "        else:\n",
    "            self.preprocessor = preprocessor\n",
    "\n",
    "        # Fit or just transform\n",
    "        if fit_preprocessor:\n",
    "            X_processed = self.preprocessor.fit_transform(X_raw)\n",
    "        else:\n",
    "            X_processed = self.preprocessor.transform(X_raw)\n",
    "\n",
    "        # Targets\n",
    "        if target is not None:\n",
    "            if pd.api.types.is_numeric_dtype(y_raw):\n",
    "                y_processed = y_raw.to_numpy().astype(np.float32) # type: ignore\n",
    "                y_tensor = torch.from_numpy(y_processed).unsqueeze(-1)  # [N, 1]\n",
    "                self.target_type_ = \"numeric\"\n",
    "                self.target_classes_ = None\n",
    "            else:\n",
    "                # Simple label encoding (no leakage of encoder across splits by default)\n",
    "                classes = sorted(y_raw.astype(str).unique()) # type: ignore\n",
    "                self.target_classes_ = {cls: i for i, cls in enumerate(classes)}\n",
    "                y_indices = y_raw.astype(str).map(self.target_classes_).to_numpy().astype(np.int64) # type: ignore\n",
    "                y_tensor = torch.from_numpy(y_indices)\n",
    "                self.target_type_ = \"categorical\"\n",
    "        else:\n",
    "            y_tensor = None\n",
    "            self.target_type_ = None\n",
    "            self.target_classes_ = None\n",
    "\n",
    "        # Store tensors\n",
    "        self.X = torch.as_tensor(X_processed, dtype=torch.float32)\n",
    "        self.y = self.X if y_tensor is None else y_tensor\n",
    "\n",
    "        # Metadata\n",
    "        self.feature_names_out_ = (\n",
    "            self.preprocessor.get_feature_names_out()\n",
    "            if hasattr(self.preprocessor, \"get_feature_names_out\")\n",
    "            else None\n",
    "        )\n",
    "        self.numeric_cols_ = numeric_cols\n",
    "        self.categorical_cols_ = categorical_cols\n",
    "\n",
    "        # Optional device move\n",
    "        self.device = device\n",
    "        if self.device is not None:\n",
    "            self.X = self.X.to(self.device)\n",
    "            self.y = self.y.to(self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input\": self.X[idx],\n",
    "            \"label\": self.y[idx],\n",
    "        }\n",
    "        \n",
    "data = pd.read_csv(FILE_PATH)\n",
    "dataset = AutoTabularDataset(\n",
    "    dataframe=data,\n",
    "    target=TARGET_COL,        # None => GAN-style (label == input)\n",
    "    preprocessor=None,        # pass a fitted preprocessor here for val/test\n",
    "    fit_preprocessor=True,\n",
    "    device=None,              # keep on CPU for DataLoader; move to DEVICE later\n",
    ")\n",
    "\n",
    "input_dim = dataset.X.shape[1]\n",
    "print(f\"Processed feature dimension: {input_dim}\")\n",
    "\n",
    "dataloader = dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True,\n",
    "                        num_workers=0, pin_memory=False, persistent_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True,\n",
    "                        num_workers=0, pin_memory=False, persistent_workers=False)\n",
    "\n",
    "model_save_freq = 100\n",
    "\n",
    "latent_dim =20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        real_data_batch = batch['input']\n",
    "        # Train discriminator on real data\n",
    "        real_labels = torch.FloatTensor(np.random.uniform(0.9, 1.0, (batch_size, 1)))\n",
    "        disc_optimizer.zero_grad()\n",
    "        output_real = discriminator(real_data_batch)\n",
    "        loss_real = criterion(output_real, real_labels)\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Train discriminator on generated data\n",
    "        fake_labels = torch.FloatTensor(np.random.uniform(0, 0.1, (batch_size, 1)))\n",
    "        noise = torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim)))\n",
    "        generated_data = generator(noise)\n",
    "        output_fake = discriminator(generated_data.detach())\n",
    "        loss_fake = criterion(output_fake, fake_labels)\n",
    "        loss_fake.backward()\n",
    "\n",
    "        disc_optimizer.step()\n",
    "\n",
    "        # Train generator \n",
    "        valid_labels = torch.FloatTensor(np.random.uniform(0.9, 1.0, (batch_size, 1)))\n",
    "        gen_optimizer.zero_grad()\n",
    "        output_g = discriminator(generated_data)\n",
    "        loss_g = criterion(output_g, valid_labels)\n",
    "        loss_g.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch}, D Loss Real: {loss_real.item()}, D Loss Fake: {loss_fake.item()}, G Loss: {loss_g.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c6b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Generate synthetic data \n",
    "synthetic_data = generator(torch.FloatTensor(np.random.normal(0, 1, (real_data.shape[0], latent_dim))))\n",
    "\n",
    "# Plot the results\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "fig.suptitle('Real and Synthetic Data Distributions', fontsize=16)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        sns.histplot(synthetic_data[:, i * 3 + j].detach().numpy(), bins=50, alpha=0.5, label='Synthetic Data', ax=axs[i, j], color='blue')\n",
    "        sns.histplot(real_data[:, i * 3 + j].numpy(), bins=50, alpha=0.5, label='Real Data', ax=axs[i, j], color='orange')\n",
    "        axs[i, j].set_title(f'Parameter {i * 3 + j + 1}', fontsize=12)\n",
    "        axs[i, j].set_xlabel('Value')\n",
    "        axs[i, j].set_ylabel('Frequency')\n",
    "        axs[i, j].legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Create a 2x3 grid of subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Comparison of Real and Synthetic Data', fontsize=16)\n",
    "\n",
    "# Define parameter names\n",
    "param_names = ['Parameter 1', 'Parameter 2', 'Parameter 3', 'Parameter 4', 'Parameter 5', 'Parameter 6']\n",
    "\n",
    "# Scatter plots for each parameter\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        param_index = i * 3 + j\n",
    "        sns.scatterplot(real_data[:, 0].numpy(), real_data[:, param_index].numpy(), label='Real Data', alpha=0.5, ax=axs[i, j])\n",
    "        sns.scatterplot(synthetic_data[:, 0].detach().numpy(), synthetic_data[:, param_index].detach().numpy(), label='Generated Data', alpha=0.5, ax=axs[i, j])\n",
    "        axs[i, j].set_title(param_names[param_index], fontsize=12)\n",
    "        axs[i, j].set_xlabel(f'Real Data - {param_names[param_index]}')\n",
    "        axs[i, j].set_ylabel(f'Real Data - {param_names[param_index]}')\n",
    "        axs[i, j].legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc02b793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]\n",
      "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "numpy 2.3.2\n",
      "pandas 2.3.1\n",
      "sklearn 1.7.1\n",
      "torch 2.8.0+cu128\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import sys, platform\n",
    "print(sys.version)\n",
    "print(platform.platform())\n",
    "\n",
    "import numpy, pandas, sklearn, torch\n",
    "print(\"numpy\", numpy.__version__)\n",
    "print(\"pandas\", pandas.__version__)\n",
    "print(\"sklearn\", sklearn.__version__)\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af5e0154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)  # must match the wheel (e.g., '12.1')\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6395a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: 2.8.0+cu128 2.3.1 2.3.2 1.7.1\n",
      "matmul done: torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch, pandas as pd, numpy as np, sklearn\n",
    "print(\"OK:\", torch.__version__, pd.__version__, np.__version__, sklearn.__version__)\n",
    "x = torch.randn(1024, 1024) @ torch.randn(1024, 1024)\n",
    "print(\"matmul done:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53bb5242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]\n",
      "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "numpy 2.3.2\n",
      "pandas 2.3.1\n",
      "sklearn 1.7.1\n",
      "torch 2.8.0+cu128\n",
      "cuda available: True\n",
      "matmul ok\n"
     ]
    }
   ],
   "source": [
    "import sys, platform\n",
    "print(sys.version)\n",
    "print(platform.platform())\n",
    "\n",
    "import numpy, pandas, sklearn, torch\n",
    "print(\"numpy\", numpy.__version__)\n",
    "print(\"pandas\", pandas.__version__)\n",
    "print(\"sklearn\", sklearn.__version__)\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "# tiny matmul to confirm native BLAS is happy\n",
    "_ = (torch.randn(512,512) @ torch.randn(512,512)).sum().item()\n",
    "print(\"matmul ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e2b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
