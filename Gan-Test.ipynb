{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecb4ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dim: torch.Size([255347, 32])\n",
      "Epoch 1: D_real=0.6777  D_fake=0.6207  G=1.0526\n",
      "Epoch 2: D_real=0.2924  D_fake=0.3845  G=1.9531\n",
      "Epoch 3: D_real=0.2740  D_fake=0.2293  G=2.6575\n"
     ]
    }
   ],
   "source": [
    "# ---- 1) Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ---- 2) Safer rare-cap and dataset\n",
    "def cap_rare_inplace(X: pd.DataFrame, cols, min_count=20, min_frac=None):\n",
    "    \"\"\"Replace infrequent categories with '_OTHER_' using .loc to avoid chained assignment.\"\"\"\n",
    "    n = len(X)\n",
    "    for col in cols:\n",
    "        s = X[col].astype(str)\n",
    "        # choose threshold\n",
    "        t = min_count\n",
    "        if min_frac is not None:\n",
    "            t = max(t, int(min_frac * n))\n",
    "        vc = s.value_counts(dropna=False)\n",
    "        rare = vc[vc < t].index\n",
    "        X.loc[:, col] = s.where(~s.isin(rare), \"_OTHER_\")\n",
    "\n",
    "class AutoTabularDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, target: str | None = None, device=None,\n",
    "                 min_count=20, min_frac=0.005):\n",
    "        df = dataframe.copy()\n",
    "\n",
    "        # Drop obvious ID-like columns (prevents 250k-wide one-hots)\n",
    "        id_like = [c for c in df.columns\n",
    "                   if c.lower().endswith(\"id\") or c.lower().endswith(\"_id\") or c.lower() == \"id\"]\n",
    "        if id_like:\n",
    "            df = df.drop(columns=id_like)\n",
    "\n",
    "        # Split X/y\n",
    "        if target is not None and target in df.columns:\n",
    "            y_raw = df[target]\n",
    "            X_raw = df.drop(columns=[target]).copy()  # copy() avoids SettingWithCopyWarning\n",
    "        else:\n",
    "            y_raw = None\n",
    "            X_raw = df.copy()\n",
    "\n",
    "        # Column types\n",
    "        num_cols = X_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        cat_cols = [c for c in X_raw.columns if c not in num_cols]\n",
    "\n",
    "        # Reduce cardinality before encoding\n",
    "        if cat_cols:\n",
    "            cap_rare_inplace(X_raw, cat_cols, min_count=min_count, min_frac=min_frac)\n",
    "\n",
    "        # Pipelines\n",
    "        num_pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", MinMaxScaler(feature_range=(-1, 1))),\n",
    "        ])\n",
    "        try:\n",
    "            cat_pipe = Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "            ])\n",
    "        except TypeError:  # older sklearn\n",
    "            cat_pipe = Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "            ])\n",
    "\n",
    "        pre = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", num_pipe, num_cols),\n",
    "                (\"cat\", cat_pipe, cat_cols),\n",
    "            ],\n",
    "            remainder=\"drop\",\n",
    "            verbose_feature_names_out=False,\n",
    "        )\n",
    "\n",
    "        Xp = pre.fit_transform(X_raw)\n",
    "\n",
    "        self.X = torch.as_tensor(Xp, dtype=torch.float32)\n",
    "        if y_raw is None:\n",
    "            self.y = self.X\n",
    "        else:\n",
    "            self.y = torch.as_tensor(y_raw.to_numpy(), dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "        self.device = device\n",
    "        if device is not None:\n",
    "            self.X = self.X.to(device)\n",
    "            self.y = self.y.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input\": self.X[idx], \"label\": self.y[idx]}\n",
    "\n",
    "# ---- 3) Load data and build loader (use .copy() when slicing)\n",
    "FILE_PATH = \"Loan_default.csv\"\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# If you want a small test first:\n",
    "# df = df.head(5000).copy()\n",
    "\n",
    "dataset = AutoTabularDataset(df, target=None, device=None,\n",
    "                             min_count=20, min_frac=0.005)\n",
    "\n",
    "print(\"Processed dim:\", dataset.X.shape)\n",
    "\n",
    "batch_size = 32  # start small; raise after stable\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, drop_last=True,\n",
    "    num_workers=0, pin_memory=False, persistent_workers=False\n",
    ")\n",
    "\n",
    "# ---- 4) Minimal models + training loop (CPU first)\n",
    "DEVICE = \"cuda\"\n",
    "latent_dim = 20\n",
    "in_dim = dataset.X.shape[1]\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z, 256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, d), nn.Tanh(),\n",
    "        )\n",
    "    def forward(self, z): return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, 256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "G = Generator(latent_dim, in_dim).to(DEVICE)\n",
    "D = Discriminator(in_dim).to(DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optD = optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optG = optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        real = batch[\"input\"].to(DEVICE)\n",
    "        bsz  = real.size(0)\n",
    "\n",
    "        # ----- Train D\n",
    "        real_lab = torch.empty(bsz, 1, device=DEVICE).uniform_(0.9, 1.0)  # label smoothing\n",
    "        fake_lab = torch.empty(bsz, 1, device=DEVICE).uniform_(0.0, 0.1)\n",
    "\n",
    "        optD.zero_grad()\n",
    "        d_real = D(real)\n",
    "        loss_real = criterion(d_real, real_lab)\n",
    "\n",
    "        z = torch.randn(bsz, latent_dim, device=DEVICE)\n",
    "        fake = G(z)\n",
    "        d_fake = D(fake.detach())\n",
    "        loss_fake = criterion(d_fake, fake_lab)\n",
    "\n",
    "        (loss_real + loss_fake).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(D.parameters(), 1.0)  # optional but helpful\n",
    "        optD.step()\n",
    "\n",
    "        # ----- Train G\n",
    "        optG.zero_grad()\n",
    "        valid = torch.empty(bsz, 1, device=DEVICE).uniform_(0.9, 1.0)\n",
    "        g_loss = criterion(D(fake), valid)\n",
    "        g_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(G.parameters(), 1.0)\n",
    "        optG.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: D_real={loss_real.item():.4f}  D_fake={loss_fake.item():.4f}  G={g_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb65022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
