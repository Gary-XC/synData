{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a71f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5837c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_rare_inplace(df: pd.DataFrame, cols, min_count=20, min_frac=0.005, other_token=\"_OTHER_\"):\n",
    "    n = len(df)\n",
    "    thr = min_count if min_frac is None else max(min_count, int(n * min_frac))\n",
    "    for c in cols:\n",
    "        s = df[c].astype(str)\n",
    "        vc = s.value_counts(dropna=False)\n",
    "        rare = vc[vc < thr].index\n",
    "        df.loc[:, c] = s.where(~s.isin(rare), other_token)\n",
    "\n",
    "def _make_onehot():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  \n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1192710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoTabularDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Unsupervised tabular dataset for GANs:\n",
    "      - Drops ID-like columns to avoid huge one-hots\n",
    "      - Imputes numeric (median) and categorical (most_frequent)\n",
    "      - Scales numeric to [-1, 1]\n",
    "      - One-hot encodes categoricals (sklearn <=/>=1.2)\n",
    "      - Stores the fitted preprocessor for reuse on new data\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, min_count=20, min_frac=0.005, feature_range=(-1, 1)):\n",
    "        df = df.copy()\n",
    "        id_like = [c for c in df.columns\n",
    "                   if c.lower().endswith(\"id\") or c.lower().endswith(\"_id\") or c.lower() == \"id\"]\n",
    "        if id_like:\n",
    "            df.drop(columns=id_like, inplace=True)\n",
    "\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        cat_cols = [c for c in df.columns if c not in num_cols]\n",
    "\n",
    "        if cat_cols:\n",
    "            cap_rare_inplace(df, cat_cols, min_count=min_count, min_frac=min_frac)\n",
    "\n",
    "        onehot = _make_onehot()\n",
    "        num_pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", MinMaxScaler(feature_range=feature_range)),\n",
    "        ])\n",
    "        cat_pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", onehot),\n",
    "        ])\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", num_pipe, num_cols),\n",
    "                (\"cat\", cat_pipe, cat_cols),\n",
    "            ],\n",
    "            remainder=\"drop\",\n",
    "            verbose_feature_names_out=False,\n",
    "        )\n",
    "\n",
    "        Xp = self.preprocessor.fit_transform(df)\n",
    "        self.X = torch.as_tensor(Xp, dtype=torch.float32)\n",
    "\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        try:\n",
    "            self.feature_names_ = self.preprocessor.get_feature_names_out().tolist()\n",
    "        except Exception:\n",
    "            self.feature_names_ = None\n",
    "\n",
    "    def __len__(self):  return self.X.shape[0]\n",
    "    def __getitem__(self, i): return {\"input\": self.X[i]}\n",
    "\n",
    "    def transform_df(self, df_new: pd.DataFrame) -> torch.Tensor:\n",
    "        df_new = df_new.copy()\n",
    "        for c in list(df_new.columns):\n",
    "            if c.lower().endswith(\"id\") or c.lower().endswith(\"_id\") or c.lower() == \"id\":\n",
    "                df_new.drop(columns=[c], inplace=True, errors=\"ignore\")\n",
    "        if self.cat_cols:\n",
    "            cap_rare_inplace(df_new, self.cat_cols, min_count=1, min_frac=None) # type: ignore\n",
    "        Xp = self.preprocessor.transform(df_new)\n",
    "        return torch.as_tensor(Xp, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad5d24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dim: torch.Size([255347, 32])\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.manual_seed_all(42)\n",
    "   \n",
    "FILE_PATH = \"../data/Loan_default.csv\"\n",
    "raw_df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "dataset = AutoTabularDataset(raw_df, min_count=20, min_frac=0.005)\n",
    "print(\"Processed dim:\", dataset.X.shape)\n",
    "\n",
    "batch_size = 256\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# ===== 3) Models (simple MLPs)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "latent_dim = 20\n",
    "in_dim = dataset.X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e0650c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z, 256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, d), nn.Tanh(),\n",
    "        )\n",
    "    def forward(self, z): return self.net(z)\n",
    "    \n",
    "G = Generator(latent_dim, in_dim).to(DEVICE)\n",
    "optG = optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f76cc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, 256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "D = Discriminator(in_dim).to(DEVICE)\n",
    "optD = optim.Adam(D.parameters(), lr=1e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55067054",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "n_critic = 2  \n",
    "noise_std = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eef2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_schedule(epoch, start=0.05, end=0.0, decay_epochs=30):\n",
    "    if epoch <= 1: return start\n",
    "    if epoch >= decay_epochs: return end\n",
    "    t = (epoch - 1) / (decay_epochs - 1)\n",
    "    return start + t * (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecb4ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: D_real=0.8856  D_fake=0.7368  G_loss=0.3445  (noise=0.050)  [scores: D(real)=0.147, D(fake)=-0.401, G_score=-0.396]\n",
      "Epoch 002: D_real=0.9885  D_fake=0.7629  G_loss=0.2871  (noise=0.048)  [scores: D(real)=-0.103, D(fake)=-0.289, G_score=-0.349]\n",
      "Epoch 003: D_real=0.9810  D_fake=0.7563  G_loss=0.2843  (noise=0.047)  [scores: D(real)=0.034, D(fake)=-0.386, G_score=-0.411]\n",
      "Epoch 004: D_real=0.9034  D_fake=0.7696  G_loss=0.2625  (noise=0.045)  [scores: D(real)=0.167, D(fake)=-0.167, G_score=-0.163]\n",
      "Epoch 005: D_real=0.8398  D_fake=0.7073  G_loss=0.3252  (noise=0.043)  [scores: D(real)=0.281, D(fake)=-0.228, G_score=-0.224]\n",
      "Epoch 006: D_real=0.8570  D_fake=0.7918  G_loss=0.2313  (noise=0.041)  [scores: D(real)=0.190, D(fake)=-0.331, G_score=-0.357]\n",
      "Epoch 007: D_real=0.9089  D_fake=0.8604  G_loss=0.1609  (noise=0.040)  [scores: D(real)=0.035, D(fake)=-0.004, G_score=-0.028]\n",
      "Epoch 008: D_real=0.9143  D_fake=0.8593  G_loss=0.1624  (noise=0.038)  [scores: D(real)=0.155, D(fake)=-0.201, G_score=-0.180]\n",
      "Epoch 009: D_real=0.8865  D_fake=0.8213  G_loss=0.1946  (noise=0.036)  [scores: D(real)=0.102, D(fake)=-0.261, G_score=-0.315]\n",
      "Epoch 010: D_real=0.7784  D_fake=0.7107  G_loss=0.3132  (noise=0.034)  [scores: D(real)=0.335, D(fake)=-0.375, G_score=-0.337]\n",
      "Epoch 011: D_real=0.6665  D_fake=0.6043  G_loss=0.4431  (noise=0.033)  [scores: D(real)=0.515, D(fake)=-0.405, G_score=-0.447]\n",
      "Epoch 012: D_real=0.5720  D_fake=0.5299  G_loss=0.5417  (noise=0.031)  [scores: D(real)=0.573, D(fake)=-0.406, G_score=-0.432]\n",
      "Epoch 013: D_real=0.5244  D_fake=0.4838  G_loss=0.5991  (noise=0.029)  [scores: D(real)=0.547, D(fake)=-0.477, G_score=-0.442]\n",
      "Epoch 014: D_real=0.5053  D_fake=0.4647  G_loss=0.6239  (noise=0.028)  [scores: D(real)=0.545, D(fake)=-0.649, G_score=-0.670]\n",
      "Epoch 015: D_real=0.4892  D_fake=0.4492  G_loss=0.6371  (noise=0.026)  [scores: D(real)=0.548, D(fake)=-0.514, G_score=-0.495]\n",
      "Epoch 016: D_real=0.4963  D_fake=0.4512  G_loss=0.6343  (noise=0.024)  [scores: D(real)=0.578, D(fake)=-0.545, G_score=-0.610]\n",
      "Epoch 017: D_real=0.5065  D_fake=0.4287  G_loss=0.6640  (noise=0.022)  [scores: D(real)=0.635, D(fake)=-0.601, G_score=-0.597]\n",
      "Epoch 018: D_real=0.5070  D_fake=0.4381  G_loss=0.6523  (noise=0.021)  [scores: D(real)=0.594, D(fake)=-0.600, G_score=-0.614]\n",
      "Epoch 019: D_real=0.4981  D_fake=0.4313  G_loss=0.6588  (noise=0.019)  [scores: D(real)=0.602, D(fake)=-0.586, G_score=-0.661]\n",
      "Epoch 020: D_real=0.4684  D_fake=0.4177  G_loss=0.6760  (noise=0.017)  [scores: D(real)=0.726, D(fake)=-0.768, G_score=-0.703]\n",
      "Epoch 021: D_real=0.4677  D_fake=0.4013  G_loss=0.6924  (noise=0.016)  [scores: D(real)=0.654, D(fake)=-0.714, G_score=-0.652]\n",
      "Epoch 022: D_real=0.4678  D_fake=0.3854  G_loss=0.7077  (noise=0.014)  [scores: D(real)=0.587, D(fake)=-0.699, G_score=-0.633]\n",
      "Epoch 023: D_real=0.4741  D_fake=0.3844  G_loss=0.7097  (noise=0.012)  [scores: D(real)=0.651, D(fake)=-0.695, G_score=-0.685]\n",
      "Epoch 024: D_real=0.4696  D_fake=0.3698  G_loss=0.7249  (noise=0.010)  [scores: D(real)=0.633, D(fake)=-0.604, G_score=-0.669]\n",
      "Epoch 025: D_real=0.4736  D_fake=0.3725  G_loss=0.7165  (noise=0.009)  [scores: D(real)=0.652, D(fake)=-0.722, G_score=-0.592]\n",
      "Epoch 026: D_real=0.4831  D_fake=0.4025  G_loss=0.6762  (noise=0.007)  [scores: D(real)=0.596, D(fake)=-0.638, G_score=-0.656]\n",
      "Epoch 027: D_real=0.5023  D_fake=0.4084  G_loss=0.6704  (noise=0.005)  [scores: D(real)=0.537, D(fake)=-0.663, G_score=-0.680]\n",
      "Epoch 028: D_real=0.5372  D_fake=0.4133  G_loss=0.6576  (noise=0.003)  [scores: D(real)=0.577, D(fake)=-0.631, G_score=-0.558]\n",
      "Epoch 029: D_real=0.5553  D_fake=0.4099  G_loss=0.6612  (noise=0.002)  [scores: D(real)=0.540, D(fake)=-0.632, G_score=-0.672]\n",
      "Epoch 030: D_real=0.5642  D_fake=0.4126  G_loss=0.6529  (noise=0.000)  [scores: D(real)=0.524, D(fake)=-0.606, G_score=-0.644]\n",
      "Epoch 031: D_real=0.5783  D_fake=0.4155  G_loss=0.6496  (noise=0.000)  [scores: D(real)=0.511, D(fake)=-0.656, G_score=-0.624]\n",
      "Epoch 032: D_real=0.5857  D_fake=0.4196  G_loss=0.6428  (noise=0.000)  [scores: D(real)=0.519, D(fake)=-0.638, G_score=-0.576]\n",
      "Epoch 033: D_real=0.5936  D_fake=0.4254  G_loss=0.6424  (noise=0.000)  [scores: D(real)=0.532, D(fake)=-0.615, G_score=-0.602]\n",
      "Epoch 034: D_real=0.6006  D_fake=0.4368  G_loss=0.6254  (noise=0.000)  [scores: D(real)=0.530, D(fake)=-0.685, G_score=-0.627]\n",
      "Epoch 035: D_real=0.6078  D_fake=0.4393  G_loss=0.6198  (noise=0.000)  [scores: D(real)=0.502, D(fake)=-0.624, G_score=-0.593]\n",
      "Epoch 036: D_real=0.6080  D_fake=0.4459  G_loss=0.6166  (noise=0.000)  [scores: D(real)=0.421, D(fake)=-0.585, G_score=-0.578]\n",
      "Epoch 037: D_real=0.6171  D_fake=0.4524  G_loss=0.6087  (noise=0.000)  [scores: D(real)=0.571, D(fake)=-0.684, G_score=-0.678]\n",
      "Epoch 038: D_real=0.6194  D_fake=0.4630  G_loss=0.5929  (noise=0.000)  [scores: D(real)=0.420, D(fake)=-0.600, G_score=-0.543]\n",
      "Epoch 039: D_real=0.6271  D_fake=0.4702  G_loss=0.5814  (noise=0.000)  [scores: D(real)=0.476, D(fake)=-0.481, G_score=-0.596]\n",
      "Epoch 040: D_real=0.6253  D_fake=0.4732  G_loss=0.5778  (noise=0.000)  [scores: D(real)=0.487, D(fake)=-0.521, G_score=-0.545]\n",
      "Epoch 041: D_real=0.6249  D_fake=0.4825  G_loss=0.5664  (noise=0.000)  [scores: D(real)=0.448, D(fake)=-0.528, G_score=-0.497]\n",
      "Epoch 042: D_real=0.6273  D_fake=0.4863  G_loss=0.5658  (noise=0.000)  [scores: D(real)=0.434, D(fake)=-0.622, G_score=-0.557]\n",
      "Epoch 043: D_real=0.6350  D_fake=0.4985  G_loss=0.5496  (noise=0.000)  [scores: D(real)=0.417, D(fake)=-0.619, G_score=-0.560]\n",
      "Epoch 044: D_real=0.6246  D_fake=0.5019  G_loss=0.5521  (noise=0.000)  [scores: D(real)=0.403, D(fake)=-0.489, G_score=-0.511]\n",
      "Epoch 045: D_real=0.6439  D_fake=0.5103  G_loss=0.5339  (noise=0.000)  [scores: D(real)=0.418, D(fake)=-0.523, G_score=-0.571]\n",
      "Epoch 046: D_real=0.6370  D_fake=0.5128  G_loss=0.5404  (noise=0.000)  [scores: D(real)=0.451, D(fake)=-0.571, G_score=-0.531]\n",
      "Epoch 047: D_real=0.6499  D_fake=0.5183  G_loss=0.5256  (noise=0.000)  [scores: D(real)=0.367, D(fake)=-0.604, G_score=-0.580]\n",
      "Epoch 048: D_real=0.6459  D_fake=0.5192  G_loss=0.5319  (noise=0.000)  [scores: D(real)=0.428, D(fake)=-0.475, G_score=-0.566]\n",
      "Epoch 049: D_real=0.6610  D_fake=0.5240  G_loss=0.5178  (noise=0.000)  [scores: D(real)=0.391, D(fake)=-0.463, G_score=-0.513]\n",
      "Epoch 050: D_real=0.6586  D_fake=0.5223  G_loss=0.5254  (noise=0.000)  [scores: D(real)=0.415, D(fake)=-0.501, G_score=-0.471]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    G.train(); D.train()\n",
    "    D_real_epoch = D_fake_epoch = G_epoch = 0.0\n",
    "    steps = 0\n",
    "    sigma = noise_schedule(epoch)\n",
    "\n",
    "    for i, batch in enumerate(loader):\n",
    "        real = batch[\"input\"].to(DEVICE)\n",
    "        b = real.size(0)\n",
    "\n",
    "        # --- D step(s)\n",
    "        for _ in range(n_critic):\n",
    "            z = torch.randn(b, latent_dim, device=DEVICE)\n",
    "            fake = G(z)\n",
    "\n",
    "            optD.zero_grad(set_to_none=True)\n",
    "\n",
    "            real_noisy = real + sigma * torch.randn_like(real)\n",
    "            fake_noisy = fake.detach() + sigma * torch.randn_like(fake)\n",
    "\n",
    "            d_real = D(real_noisy)\n",
    "            d_fake = D(fake_noisy)\n",
    "\n",
    "            hinge_D = F.relu(1.0 - d_real).mean() + F.relu(1.0 + d_fake).mean()\n",
    "\n",
    "            # Lazy R1 (Î³=1.0) on real every 16 steps\n",
    "            if i % 16 == 0:\n",
    "                x_real = real.detach().requires_grad_(True)\n",
    "                d_sum = D(x_real).sum()\n",
    "                grads = torch.autograd.grad(d_sum, x_real, create_graph=True)[0]\n",
    "                r1 = grads.pow(2).sum(dim=1).mean() * 1.0\n",
    "                loss_D_total = hinge_D + r1\n",
    "            else:\n",
    "                loss_D_total = hinge_D\n",
    "\n",
    "            loss_D_total.backward()\n",
    "            optD.step()\n",
    "\n",
    "        # --- G step\n",
    "        z = torch.randn(b, latent_dim, device=DEVICE)\n",
    "        fake = G(z)\n",
    "        optG.zero_grad(set_to_none=True)\n",
    "        loss_G = -D(fake).mean()\n",
    "        loss_G.backward()\n",
    "        optG.step()\n",
    "\n",
    "        # Accumulate epoch metrics (hinge penalties; lower is better)\n",
    "        with torch.no_grad():\n",
    "            D_real_epoch += F.relu(1 - d_real).mean().item()\n",
    "            D_fake_epoch += F.relu(1 + d_fake).mean().item()\n",
    "            G_epoch      += loss_G.item()\n",
    "            steps += 1\n",
    "\n",
    "    # epoch averages\n",
    "    D_real_avg = D_real_epoch / steps\n",
    "    D_fake_avg = D_fake_epoch / steps\n",
    "    G_avg      = G_epoch / steps\n",
    "\n",
    "    # Optional raw score peek on the *last* batch\n",
    "    D_real_score = d_real.mean().item()\n",
    "    D_fake_score = d_fake.mean().item()\n",
    "    G_score = -loss_G.item()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d}: \"\n",
    "        f\"D_real={D_real_avg:.4f}  D_fake={D_fake_avg:.4f}  G_loss={G_avg:.4f}  \"\n",
    "        f\"(noise={sigma:.3f})  \"\n",
    "        f\"[scores: D(real)={D_real_score:.3f}, D(fake)={D_fake_score:.3f}, G_score={G_score:.3f}]\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
