{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a71f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5837c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- helpers\n",
    "def cap_rare_inplace(df: pd.DataFrame, cols, min_count=20, min_frac=0.005, other_token=\"_OTHER_\"):\n",
    "    \"\"\"Replace infrequent categories with 'other_token' (in-place).\"\"\"\n",
    "    n = len(df)\n",
    "    thr = min_count if min_frac is None else max(min_count, int(n * min_frac))\n",
    "    for c in cols:\n",
    "        s = df[c].astype(str)\n",
    "        vc = s.value_counts(dropna=False)\n",
    "        rare = vc[vc < thr].index\n",
    "        df.loc[:, c] = s.where(~s.isin(rare), other_token)\n",
    "\n",
    "def _make_onehot():\n",
    "    \"\"\"Create OneHotEncoder compatible with both old/new sklearn.\"\"\"\n",
    "    try:\n",
    "        # sklearn >= 1.2\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        # sklearn < 1.2\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1192710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- dataset\n",
    "class AutoTabularDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Unsupervised tabular dataset for GANs:\n",
    "      - Drops ID-like columns to avoid huge one-hots\n",
    "      - Imputes numeric (median) and categorical (most_frequent)\n",
    "      - Scales numeric to [-1, 1]\n",
    "      - One-hot encodes categorically (compat with sklearn <1.2 and >=1.2)\n",
    "      - Stores the fitted preprocessor for reuse on new data\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, min_count=20, min_frac=0.005, feature_range=(-1, 1)):\n",
    "        df = df.copy()\n",
    "\n",
    "        # 1) drop obvious ID-like columns\n",
    "        id_like = [c for c in df.columns\n",
    "                   if c.lower().endswith(\"id\") or c.lower().endswith(\"_id\") or c.lower() == \"id\"]\n",
    "        if id_like:\n",
    "            df.drop(columns=id_like, inplace=True)\n",
    "\n",
    "        # 2) split by dtype\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        cat_cols = [c for c in df.columns if c not in num_cols]\n",
    "\n",
    "        # 3) reduce categorical cardinality before encoding\n",
    "        if cat_cols:\n",
    "            cap_rare_inplace(df, cat_cols, min_count=min_count, min_frac=min_frac)\n",
    "\n",
    "        # 4) build preprocessor\n",
    "        onehot = _make_onehot()\n",
    "        num_pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", MinMaxScaler(feature_range=feature_range)),\n",
    "        ])\n",
    "        cat_pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", onehot),\n",
    "        ])\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", num_pipe, num_cols),\n",
    "                (\"cat\", cat_pipe, cat_cols),\n",
    "            ],\n",
    "            remainder=\"drop\",\n",
    "            verbose_feature_names_out=False,\n",
    "        )\n",
    "\n",
    "        # 5) fit + transform\n",
    "        Xp = self.preprocessor.fit_transform(df)\n",
    "        self.X = torch.as_tensor(Xp, dtype=torch.float32)\n",
    "\n",
    "        # keep some metadata (optional but handy)\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        try:\n",
    "            self.feature_names_ = self.preprocessor.get_feature_names_out().tolist()\n",
    "        except Exception:\n",
    "            self.feature_names_ = None\n",
    "\n",
    "    # ---- torch Dataset protocol\n",
    "    def __len__(self):  return self.X.shape[0]\n",
    "    def __getitem__(self, i): return {\"input\": self.X[i]}\n",
    "\n",
    "    # ---- transform new data later with the SAME preprocessor\n",
    "    def transform_df(self, df_new: pd.DataFrame) -> torch.Tensor:\n",
    "        df_new = df_new.copy()\n",
    "        # drop the same id-like columns if present\n",
    "        for c in list(df_new.columns):\n",
    "            if c.lower().endswith(\"id\") or c.lower().endswith(\"_id\") or c.lower() == \"id\":\n",
    "                df_new.drop(columns=[c], inplace=True, errors=\"ignore\")\n",
    "        # cap rare on same categorical columns (prevent unseen blowups)\n",
    "        if self.cat_cols:\n",
    "            cap_rare_inplace(df_new, self.cat_cols, min_count=1, min_frac=None) # type: ignore\n",
    "        Xp = self.preprocessor.transform(df_new)\n",
    "        return torch.as_tensor(Xp, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad5d24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dim: torch.Size([255347, 32])\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.manual_seed_all(42)\n",
    "   \n",
    "FILE_PATH = \"Loan_default.csv\"\n",
    "raw_df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "dataset = AutoTabularDataset(raw_df, min_count=20, min_frac=0.005)\n",
    "print(\"Processed dim:\", dataset.X.shape)\n",
    "\n",
    "batch_size = 256\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# ===== 3) Models (simple MLPs)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "latent_dim = 20\n",
    "in_dim = dataset.X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0650c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z, 256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, d), nn.Tanh(),\n",
    "        )\n",
    "    def forward(self, z): return self.net(z)\n",
    "    \n",
    "G = Generator(latent_dim, in_dim).to(DEVICE)\n",
    "optG = optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f76cc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, 256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1),  # scores (not probabilities)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "    \n",
    "D = Discriminator(in_dim).to(DEVICE)\n",
    "optD = optim.Adam(D.parameters(), lr=1e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55067054",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "n_critic = 2  # D steps per G step (typical for hinge)\n",
    "noise_std = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecb4ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: D_real=0.8727  D_fake=0.6213  G_loss=0.3962  [scores: D(real)=0.147, D(fake)=-0.401, G_score=-0.396]\n",
      "Epoch 002: D_real=1.1091  D_fake=0.7336  G_loss=0.3368  [scores: D(real)=-0.104, D(fake)=-0.275, G_score=-0.337]\n",
      "Epoch 003: D_real=0.9746  D_fake=0.7120  G_loss=0.3251  [scores: D(real)=0.042, D(fake)=-0.297, G_score=-0.325]\n",
      "Epoch 004: D_real=0.8818  D_fake=0.7036  G_loss=0.3424  [scores: D(real)=0.125, D(fake)=-0.307, G_score=-0.342]\n",
      "Epoch 005: D_real=0.8138  D_fake=0.6941  G_loss=0.3558  [scores: D(real)=0.207, D(fake)=-0.316, G_score=-0.356]\n",
      "Epoch 006: D_real=0.9090  D_fake=1.0186  G_loss=-0.0373  [scores: D(real)=0.093, D(fake)=0.017, G_score=0.037]\n",
      "Epoch 007: D_real=0.9147  D_fake=0.7694  G_loss=0.2656  [scores: D(real)=0.088, D(fake)=-0.235, G_score=-0.266]\n",
      "Epoch 008: D_real=0.9337  D_fake=0.8629  G_loss=0.1231  [scores: D(real)=0.067, D(fake)=-0.137, G_score=-0.123]\n",
      "Epoch 009: D_real=0.9470  D_fake=0.7833  G_loss=0.1953  [scores: D(real)=0.057, D(fake)=-0.224, G_score=-0.195]\n",
      "Epoch 010: D_real=0.7600  D_fake=0.8326  G_loss=0.2658  [scores: D(real)=0.260, D(fake)=-0.179, G_score=-0.266]\n",
      "Epoch 011: D_real=0.7524  D_fake=0.7447  G_loss=0.2450  [scores: D(real)=0.260, D(fake)=-0.275, G_score=-0.245]\n",
      "Epoch 012: D_real=0.6827  D_fake=0.6602  G_loss=0.3644  [scores: D(real)=0.353, D(fake)=-0.368, G_score=-0.364]\n",
      "Epoch 013: D_real=0.6080  D_fake=0.6325  G_loss=0.3967  [scores: D(real)=0.445, D(fake)=-0.411, G_score=-0.397]\n",
      "Epoch 014: D_real=0.5791  D_fake=0.5258  G_loss=0.4790  [scores: D(real)=0.500, D(fake)=-0.545, G_score=-0.479]\n",
      "Epoch 015: D_real=0.6826  D_fake=0.6390  G_loss=0.4146  [scores: D(real)=0.365, D(fake)=-0.410, G_score=-0.415]\n",
      "Epoch 016: D_real=0.5055  D_fake=0.4195  G_loss=0.6629  [scores: D(real)=0.626, D(fake)=-0.672, G_score=-0.663]\n",
      "Epoch 017: D_real=0.5064  D_fake=0.4534  G_loss=0.6633  [scores: D(real)=0.590, D(fake)=-0.648, G_score=-0.663]\n",
      "Epoch 018: D_real=0.4680  D_fake=0.5664  G_loss=0.5062  [scores: D(real)=0.646, D(fake)=-0.484, G_score=-0.506]\n",
      "Epoch 019: D_real=0.4546  D_fake=0.5740  G_loss=0.4949  [scores: D(real)=0.652, D(fake)=-0.481, G_score=-0.495]\n",
      "Epoch 020: D_real=0.5800  D_fake=0.5825  G_loss=0.4577  [scores: D(real)=0.481, D(fake)=-0.472, G_score=-0.458]\n",
      "Epoch 021: D_real=0.5641  D_fake=0.5101  G_loss=0.4565  [scores: D(real)=0.512, D(fake)=-0.545, G_score=-0.456]\n",
      "Epoch 022: D_real=0.5634  D_fake=0.5158  G_loss=0.4282  [scores: D(real)=0.498, D(fake)=-0.544, G_score=-0.428]\n",
      "Epoch 023: D_real=0.6013  D_fake=0.5077  G_loss=0.5121  [scores: D(real)=0.476, D(fake)=-0.545, G_score=-0.512]\n",
      "Epoch 024: D_real=0.4971  D_fake=0.5271  G_loss=0.5311  [scores: D(real)=0.559, D(fake)=-0.527, G_score=-0.531]\n",
      "Epoch 025: D_real=0.5734  D_fake=0.4967  G_loss=0.4890  [scores: D(real)=0.486, D(fake)=-0.557, G_score=-0.489]\n",
      "Epoch 026: D_real=0.5104  D_fake=0.5379  G_loss=0.4407  [scores: D(real)=0.542, D(fake)=-0.511, G_score=-0.441]\n",
      "Epoch 027: D_real=0.6279  D_fake=0.5365  G_loss=0.3822  [scores: D(real)=0.403, D(fake)=-0.533, G_score=-0.382]\n",
      "Epoch 028: D_real=0.6227  D_fake=0.5873  G_loss=0.3970  [scores: D(real)=0.413, D(fake)=-0.445, G_score=-0.397]\n",
      "Epoch 029: D_real=0.6468  D_fake=0.5974  G_loss=0.3217  [scores: D(real)=0.381, D(fake)=-0.424, G_score=-0.322]\n",
      "Epoch 030: D_real=0.5654  D_fake=0.5845  G_loss=0.3907  [scores: D(real)=0.485, D(fake)=-0.451, G_score=-0.391]\n",
      "Epoch 031: D_real=0.6164  D_fake=0.6402  G_loss=0.2608  [scores: D(real)=0.414, D(fake)=-0.392, G_score=-0.261]\n",
      "Epoch 032: D_real=0.7115  D_fake=0.7256  G_loss=0.2712  [scores: D(real)=0.304, D(fake)=-0.293, G_score=-0.271]\n",
      "Epoch 033: D_real=0.6916  D_fake=0.6180  G_loss=0.2713  [scores: D(real)=0.328, D(fake)=-0.415, G_score=-0.271]\n",
      "Epoch 034: D_real=0.6507  D_fake=0.6566  G_loss=0.3339  [scores: D(real)=0.375, D(fake)=-0.375, G_score=-0.334]\n",
      "Epoch 035: D_real=0.6996  D_fake=0.6931  G_loss=0.3080  [scores: D(real)=0.334, D(fake)=-0.321, G_score=-0.308]\n",
      "Epoch 036: D_real=0.7074  D_fake=0.7358  G_loss=0.2162  [scores: D(real)=0.317, D(fake)=-0.291, G_score=-0.216]\n",
      "Epoch 037: D_real=0.6561  D_fake=0.6333  G_loss=0.3522  [scores: D(real)=0.379, D(fake)=-0.400, G_score=-0.352]\n",
      "Epoch 038: D_real=0.7265  D_fake=0.6312  G_loss=0.2736  [scores: D(real)=0.301, D(fake)=-0.396, G_score=-0.274]\n",
      "Epoch 039: D_real=0.7231  D_fake=0.6781  G_loss=0.2019  [scores: D(real)=0.299, D(fake)=-0.347, G_score=-0.202]\n",
      "Epoch 040: D_real=0.7209  D_fake=0.7853  G_loss=0.2088  [scores: D(real)=0.301, D(fake)=-0.227, G_score=-0.209]\n",
      "Epoch 041: D_real=0.7456  D_fake=0.6477  G_loss=0.2093  [scores: D(real)=0.281, D(fake)=-0.389, G_score=-0.209]\n",
      "Epoch 042: D_real=0.6442  D_fake=0.6672  G_loss=0.1842  [scores: D(real)=0.393, D(fake)=-0.368, G_score=-0.184]\n",
      "Epoch 043: D_real=0.7309  D_fake=0.6791  G_loss=0.2329  [scores: D(real)=0.293, D(fake)=-0.347, G_score=-0.233]\n",
      "Epoch 044: D_real=0.7270  D_fake=0.7323  G_loss=0.2053  [scores: D(real)=0.304, D(fake)=-0.287, G_score=-0.205]\n",
      "Epoch 045: D_real=0.6970  D_fake=0.6542  G_loss=0.2774  [scores: D(real)=0.334, D(fake)=-0.365, G_score=-0.277]\n",
      "Epoch 046: D_real=0.7704  D_fake=0.6373  G_loss=0.4298  [scores: D(real)=0.254, D(fake)=-0.385, G_score=-0.430]\n",
      "Epoch 047: D_real=0.7024  D_fake=0.6651  G_loss=0.2488  [scores: D(real)=0.322, D(fake)=-0.351, G_score=-0.249]\n",
      "Epoch 048: D_real=0.7356  D_fake=0.6765  G_loss=0.4223  [scores: D(real)=0.283, D(fake)=-0.348, G_score=-0.422]\n",
      "Epoch 049: D_real=0.8010  D_fake=0.6941  G_loss=0.2298  [scores: D(real)=0.212, D(fake)=-0.318, G_score=-0.230]\n",
      "Epoch 050: D_real=0.7540  D_fake=0.6996  G_loss=0.2894  [scores: D(real)=0.278, D(fake)=-0.321, G_score=-0.289]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    D_real_epoch, D_fake_epoch, G_epoch, steps = 0.0, 0.0, 0.0, 0\n",
    "\n",
    "    for i, batch in enumerate(loader):\n",
    "        real = batch[\"input\"].to(DEVICE)\n",
    "        b = real.size(0)\n",
    "\n",
    "        # --- D step(s)\n",
    "        for _ in range(n_critic):\n",
    "            z = torch.randn(b, latent_dim, device=DEVICE)\n",
    "            fake = G(z)\n",
    "\n",
    "            optD.zero_grad(set_to_none=True)\n",
    "            \n",
    "            real_noisy = real + noise_std * torch.randn_like(real)\n",
    "            fake_noisy = fake.detach() + noise_std * torch.randn_like(fake)\n",
    "\n",
    "            d_real = D(real_noisy)\n",
    "            d_fake = D(fake_noisy)\n",
    "\n",
    "            # Hinge loss for D: E[max(0, 1 - D(real))] + E[max(0, 1 + D(fake))]\n",
    "            loss_D = F.relu(1.0 - d_real).mean() + F.relu(1.0 + d_fake).mean()\n",
    "            \n",
    "            if i % 16 == 0:\n",
    "                x_real = real.detach().requires_grad_(True)\n",
    "                d_real_full = D(x_real).sum()\n",
    "                grads = torch.autograd.grad(d_real_full, x_real, create_graph=True)[0]\n",
    "                r1_penalty = grads.pow(2).sum(dim=1).mean() * 1.0  \n",
    "                r1_penalty.backward()\n",
    "            \n",
    "            loss_D.backward()\n",
    "            optD.step()\n",
    "\n",
    "        # --- G step\n",
    "        z = torch.randn(b, latent_dim, device=DEVICE)\n",
    "        fake = G(z)\n",
    "\n",
    "        optG.zero_grad(set_to_none=True)\n",
    "        # Hinge loss for G: -E[D(fake)]\n",
    "        loss_G = -D(fake).mean()\n",
    "        loss_G.backward()\n",
    "        optG.step()\n",
    "\n",
    "        # track averages\n",
    "        D_real_epoch += (1.0 - d_real.detach()).clamp_min(0).mean().item()\n",
    "        D_fake_epoch += (1.0 + d_fake.detach()).clamp_min(0).mean().item()\n",
    "        G_epoch      += loss_G.detach().item()\n",
    "        steps += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            D_real_term = torch.relu(torch.ones_like(d_real) - d_real).mean().item()\n",
    "            D_fake_term = torch.relu(torch.ones_like(d_fake) + d_fake).mean().item()\n",
    "\n",
    "            # Raw scores (for intuition)\n",
    "            D_real_score = d_real.mean().item()\n",
    "            D_fake_score = d_fake.mean().item()\n",
    "\n",
    "            # Generator objectives\n",
    "            G_loss_val = loss_G.detach().item()   # you already computed loss_G above\n",
    "            G_score    = -G_loss_val              # because loss_G = -mean(D(fake))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d}: \"\n",
    "        f\"D_real={D_real_term:.4f}  D_fake={D_fake_term:.4f}  G_loss={G_loss_val:.4f}  \"\n",
    "        f\"[scores: D(real)={D_real_score:.3f}, D(fake)={D_fake_score:.3f}, G_score={G_score:.3f}]\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
