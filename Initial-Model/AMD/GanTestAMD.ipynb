{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9505c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DEVICE: privateuseone:1\n",
      "Epoch   1: D_real=0.1169  D_fake=0.0004  G=0.9775\n",
      "Epoch   2: D_real=0.0329  D_fake=0.3003  G=1.5659\n",
      "Epoch   3: D_real=0.0288  D_fake=0.0006  G=0.9104\n",
      "Epoch   4: D_real=0.0540  D_fake=0.0028  G=0.9746\n",
      "Epoch   5: D_real=0.0545  D_fake=0.0048  G=0.9056\n",
      "Epoch   6: D_real=0.0353  D_fake=0.0087  G=0.9694\n",
      "Epoch   7: D_real=0.0359  D_fake=0.0058  G=1.0218\n",
      "Epoch   8: D_real=0.0483  D_fake=0.1075  G=1.1257\n",
      "Epoch   9: D_real=0.0414  D_fake=0.0013  G=0.9404\n",
      "Epoch  10: D_real=0.0463  D_fake=0.0492  G=1.2766\n",
      "Epoch  11: D_real=0.0609  D_fake=0.0061  G=0.9574\n",
      "Epoch  12: D_real=0.0528  D_fake=0.2096  G=1.7349\n",
      "Epoch  13: D_real=0.7190  D_fake=0.0001  G=0.9560\n",
      "Epoch  14: D_real=0.0523  D_fake=0.0001  G=0.9745\n",
      "Epoch  15: D_real=0.0935  D_fake=0.0286  G=1.0002\n",
      "Epoch  16: D_real=0.1058  D_fake=0.0230  G=0.9989\n",
      "Epoch  17: D_real=0.0409  D_fake=0.0100  G=0.8833\n",
      "Epoch  18: D_real=0.0543  D_fake=0.0025  G=0.9184\n",
      "Epoch  19: D_real=0.0520  D_fake=0.0009  G=0.9407\n",
      "Epoch  20: D_real=0.0308  D_fake=0.1217  G=1.2116\n",
      "Epoch  21: D_real=0.0254  D_fake=0.1373  G=1.1716\n",
      "Epoch  22: D_real=0.0253  D_fake=0.0027  G=0.9657\n",
      "Epoch  23: D_real=0.0270  D_fake=0.0196  G=1.0261\n",
      "Epoch  24: D_real=0.0212  D_fake=0.0499  G=1.2054\n",
      "Epoch  25: D_real=0.2124  D_fake=0.0008  G=1.0371\n",
      "Epoch  26: D_real=0.0257  D_fake=0.0004  G=0.9890\n",
      "Epoch  27: D_real=0.0659  D_fake=0.0019  G=0.8897\n",
      "Epoch  28: D_real=0.0081  D_fake=0.0018  G=0.9950\n",
      "Epoch  29: D_real=0.0149  D_fake=0.0012  G=0.9392\n",
      "Epoch  30: D_real=0.0049  D_fake=0.0003  G=1.0267\n",
      "Saved synthetic_preview.csv (first 100 rows)\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# DirectML Tabular GAN — GPU-Resident (manual batching)\n",
    "# ===========================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- Imports\n",
    "import random, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch_directml as dml\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Reproducibility\n",
    "# ---------------------------\n",
    "SEED = 999\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Device (prefer dGPU:1)\n",
    "# ---------------------------\n",
    "try:\n",
    "    DEVICE = dml.device(1)      # your dedicated GPU index; change if needed\n",
    "    _ = torch.ones(1, device=DEVICE)\n",
    "except Exception:\n",
    "    # fallback: try 0, else CPU\n",
    "    try:\n",
    "        DEVICE = dml.device(0); _ = torch.ones(1, device=DEVICE)\n",
    "    except Exception:\n",
    "        DEVICE = \"cpu\"\n",
    "print(\"Using DEVICE:\", DEVICE)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 2) Preprocessing helpers + Dataset (inverse)\n",
    "# --------------------------------------------\n",
    "def cap_rare_inplace(X: pd.DataFrame, cols, min_count=20, min_frac=None):\n",
    "    n = len(X)\n",
    "    for col in cols:\n",
    "        s = X[col].astype(str)\n",
    "        t = min_count if min_frac is None else max(min_count, int(min_frac * n))\n",
    "        vc = s.value_counts(dropna=False)\n",
    "        rare = vc[vc < t].index\n",
    "        X.loc[:, col] = s.where(~s.isin(rare), \"_OTHER_\")\n",
    "\n",
    "class AutoTabularDataset:\n",
    "    \"\"\"Simple holder with fitted preprocessor + tensors.\"\"\"\n",
    "    def __init__(self, dataframe: pd.DataFrame, target: str | None = None,\n",
    "                 min_count=100, min_frac=0.01):\n",
    "        df = dataframe.copy()\n",
    "\n",
    "        # Drop obvious IDs\n",
    "        id_like = [c for c in df.columns\n",
    "                   if c.lower().endswith(\"id\") or c.lower().endswith(\"_id\") or c.lower()==\"id\"]\n",
    "        if id_like: df = df.drop(columns=id_like)\n",
    "\n",
    "        # X/y split\n",
    "        if target is not None and target in df.columns:\n",
    "            y_raw = df[target]; X_raw = df.drop(columns=[target]).copy()\n",
    "        else:\n",
    "            y_raw = None; X_raw = df.copy()\n",
    "\n",
    "        # Types\n",
    "        self.num_cols = X_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.cat_cols = [c for c in X_raw.columns if c not in self.num_cols]\n",
    "\n",
    "        # Rare-cap\n",
    "        if self.cat_cols:\n",
    "            cap_rare_inplace(X_raw, self.cat_cols, min_count=min_count, min_frac=min_frac)\n",
    "\n",
    "        # Pipelines\n",
    "        num_pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", MinMaxScaler(feature_range=(-1, 1))),\n",
    "        ])\n",
    "        try:\n",
    "            cat_pipe = Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, dtype=np.float32)),\n",
    "            ])\n",
    "        except TypeError:\n",
    "            cat_pipe = Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False, dtype=np.float32)),\n",
    "            ])\n",
    "\n",
    "        self.pre = ColumnTransformer(\n",
    "            transformers=[(\"num\", num_pipe, self.num_cols),\n",
    "                          (\"cat\", cat_pipe, self.cat_cols)],\n",
    "            remainder=\"drop\", verbose_feature_names_out=False\n",
    "        )\n",
    "\n",
    "        Xp = self.pre.fit_transform(X_raw).astype(np.float32)\n",
    "        self.X = torch.as_tensor(Xp, dtype=torch.float32, device=\"cpu\")\n",
    "\n",
    "        # One-hot group sizes (for inverse)\n",
    "        self.cat_group_sizes = []\n",
    "        if self.cat_cols:\n",
    "            oh: OneHotEncoder = self.pre.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "            for cats in oh.categories_:\n",
    "                self.cat_group_sizes.append(len(cats))\n",
    "\n",
    "    def _harden_onehots(self, X_fake_np: np.ndarray) -> np.ndarray:\n",
    "        if not self.cat_cols: return X_fake_np\n",
    "        out = X_fake_np.copy()\n",
    "        num_dim = len(self.num_cols)\n",
    "        start = num_dim\n",
    "        for g in self.cat_group_sizes:\n",
    "            if g <= 0: continue\n",
    "            block = out[:, start:start+g]\n",
    "            idx = np.argmax(block, axis=1)\n",
    "            block[:] = 0.0\n",
    "            block[np.arange(block.shape[0]), idx] = 1.0\n",
    "            out[:, start:start+g] = block\n",
    "            start += g\n",
    "        return out\n",
    "\n",
    "    def inverse_to_dataframe(self, X_fake: torch.Tensor) -> pd.DataFrame:\n",
    "        Xf = X_fake.detach().to(\"cpu\").numpy().astype(np.float32)\n",
    "        if self.num_cols:\n",
    "            num_dim = len(self.num_cols)\n",
    "            Xf[:, :num_dim] = np.clip(Xf[:, :num_dim], -1.0, 1.0)\n",
    "        Xf = self._harden_onehots(Xf)\n",
    "        try:\n",
    "            X_inv = self.pre.inverse_transform(Xf)\n",
    "            cols = self.num_cols + self.cat_cols\n",
    "            return pd.DataFrame(X_inv, columns=cols)\n",
    "        except Exception:\n",
    "            cols = [f\"f{i}\" for i in range(Xf.shape[1])]\n",
    "            return pd.DataFrame(Xf, columns=cols)\n",
    "\n",
    "# ------------------------\n",
    "# 3) Load data\n",
    "# ------------------------\n",
    "FILE_PATH = \"../data/Loan_default.csv\"\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "dataset = AutoTabularDataset(df, target=None, min_count=100, min_frac=0.01)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4) Move the WHOLE dataset to GPU once (big speedup)\n",
    "# ----------------------------------------------------\n",
    "X_device = dataset.X.to(DEVICE, non_blocking=True)  # one-time host→device copy\n",
    "N, in_dim = X_device.shape\n",
    "latent_dim = 64\n",
    "\n",
    "# ------------------------\n",
    "# 5) Models (heavier nets)\n",
    "# ------------------------\n",
    "width = 2048  # raise/lower based on VRAM\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z, width), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(width, width), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(width, width), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(width, width), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(width, d), nn.Tanh(),\n",
    "        )\n",
    "    def forward(self, z): return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, width), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(width, width), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(width, width), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(width, width), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(width, 1),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "G = Generator(latent_dim, in_dim).to(DEVICE)\n",
    "D = Discriminator(in_dim).to(DEVICE)\n",
    "\n",
    "# --------------------------------\n",
    "# 6) Loss, optimizers, (optional EMA)\n",
    "# --------------------------------\n",
    "criterion = nn.MSELoss()  # LSGAN: real=1, fake=0\n",
    "optD = optim.Adam(D.parameters(), lr=3e-4, betas=(0.0, 0.99))\n",
    "optG = optim.Adam(G.parameters(), lr=1e-4, betas=(0.0, 0.99))\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
    "        self.keys = list(self.shadow.keys()); self.model = model\n",
    "    @torch.no_grad()\n",
    "    def update(self):\n",
    "        msd = self.model.state_dict()\n",
    "        for k in self.keys:\n",
    "            self.shadow[k].lerp_(msd[k].detach(), 1.0 - self.decay)\n",
    "    def copy_to(self, model: nn.Module): model.load_state_dict(self.shadow, strict=False)\n",
    "\n",
    "ema = EMA(G, decay=0.999)\n",
    "\n",
    "# -----------------------------\n",
    "# 7) GPU-resident training loop\n",
    "# -----------------------------\n",
    "batch_size = 4096   # try 2048 if it fits; reduce on OOM\n",
    "num_epochs = 30\n",
    "\n",
    "# Preallocate labels max size (slice each step)\n",
    "ones_full  = torch.ones(batch_size, 1, device=DEVICE)\n",
    "zeros_full = torch.zeros(batch_size, 1, device=DEVICE)\n",
    "\n",
    "G.train(); D.train()\n",
    "torch.set_grad_enabled(True)     # global switch back ON\n",
    "for epoch in range(num_epochs):\n",
    "    # shuffle indices ON GPU\n",
    "    perm = torch.randperm(N, device=DEVICE)\n",
    "    for i in range(0, N, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        real = X_device.index_select(0, idx)\n",
    "        bsz = real.size(0)\n",
    "\n",
    "        real_lab = ones_full[:bsz]\n",
    "        fake_lab = zeros_full[:bsz]\n",
    "\n",
    "        # -- D step\n",
    "        optD.zero_grad(set_to_none=True)\n",
    "        d_real = D(real); loss_real = criterion(d_real, real_lab)\n",
    "\n",
    "        z = torch.randn(bsz, latent_dim, device=DEVICE)\n",
    "        fake = G(z)\n",
    "        d_fake = D(fake.detach()); loss_fake = criterion(d_fake, fake_lab)\n",
    "\n",
    "        (loss_real + loss_fake).backward()\n",
    "        optD.step()\n",
    "\n",
    "        # -- G step\n",
    "        optG.zero_grad(set_to_none=True)\n",
    "        g_loss = criterion(D(fake), real_lab)\n",
    "        g_loss.backward()\n",
    "        optG.step()\n",
    "        ema.update()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:3d}: D_real={loss_real.item():.4f}  D_fake={loss_fake.item():.4f}  G={g_loss.item():.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 8) Sampling + inverse back to DF\n",
    "# -------------------------------\n",
    "G_eval = Generator(latent_dim, in_dim).to(DEVICE)\n",
    "ema.copy_to(G_eval); G_eval.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(1000, latent_dim, device=DEVICE)\n",
    "    X_fake = G_eval(z)\n",
    "\n",
    "synthetic_df = dataset.inverse_to_dataframe(X_fake)\n",
    "synthetic_df.head(100).to_csv(\"synthetic_preview.csv\", index=False)\n",
    "print(\"Saved synthetic_preview.csv (first 100 rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6261741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DEVICE: privateuseone:1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "m_device->CreateOperator(&opDesc, IID_PPV_ARGS(&op))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 222\u001b[39m\n\u001b[32m    219\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[32m256\u001b[39m, b // step)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m batch_size = \u001b[43mfind_max_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8192\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;66;03m# Preallocate big buffers (slice per step)\u001b[39;00m\n\u001b[32m    224\u001b[39m ones_full  = torch.full((batch_size, \u001b[32m1\u001b[39m), \u001b[32m0.95\u001b[39m, device=DEVICE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 211\u001b[39m, in \u001b[36mfind_max_batch\u001b[39m\u001b[34m(start, limit, step)\u001b[39m\n\u001b[32m    209\u001b[39m tmp_z     = torch.randn(b_eff, latent_dim, device=DEVICE)\n\u001b[32m    210\u001b[39m D.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m); G.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_real\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_ones\u001b[49m\u001b[43m)\u001b[49m + criterion(D(G(tmp_z).detach()), tmp_zeros)\n\u001b[32m    212\u001b[39m loss.backward()  \u001b[38;5;66;03m# probe memory use\u001b[39;00m\n\u001b[32m    213\u001b[39m b *= step\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/dml/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/dml/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/dml/lib/python3.12/site-packages/torch/nn/modules/loss.py:538\u001b[39m, in \u001b[36mMSELoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/dml/lib/python3.12/site-packages/torch/nn/functional.py:3384\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction)\u001b[39m\n\u001b[32m   3381\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m   3383\u001b[39m expanded_input, expanded_target = torch.broadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[32m-> \u001b[39m\u001b[32m3384\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: m_device->CreateOperator(&opDesc, IID_PPV_ARGS(&op))"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# DirectML Tabular GAN — GPU-Resident (fast & stable, drop-in)\n",
    "# ===============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- Imports\n",
    "import random, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch_directml as dml\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Reproducibility\n",
    "# ---------------------------\n",
    "SEED = 999\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Device (prefer dGPU:1)\n",
    "# ---------------------------\n",
    "try:\n",
    "    DEVICE = dml.device(1)      # your dedicated GPU index; change if needed\n",
    "    _ = torch.ones(1, device=DEVICE)\n",
    "except Exception:\n",
    "    try:\n",
    "        DEVICE = dml.device(0); _ = torch.ones(1, device=DEVICE)\n",
    "    except Exception:\n",
    "        DEVICE = \"cpu\"\n",
    "print(\"Using DEVICE:\", DEVICE)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 2) Preprocessing helpers + Dataset (inverse)\n",
    "# --------------------------------------------\n",
    "def cap_rare_inplace(X: pd.DataFrame, cols, min_count=20, min_frac=None):\n",
    "    n = len(X)\n",
    "    for col in cols:\n",
    "        s = X[col].astype(str)\n",
    "        t = min_count if min_frac is None else max(min_count, int(min_frac * n))\n",
    "        vc = s.value_counts(dropna=False)\n",
    "        rare = vc[vc < t].index\n",
    "        X.loc[:, col] = s.where(~s.isin(rare), \"_OTHER_\")\n",
    "\n",
    "class AutoTabularDataset:\n",
    "    \"\"\"Simple holder with fitted preprocessor + tensors (keeps inverse mapping).\"\"\"\n",
    "    def __init__(self, dataframe: pd.DataFrame, target: str | None = None,\n",
    "                 min_count=100, min_frac=0.01):\n",
    "        df = dataframe.copy()\n",
    "\n",
    "        # Drop obvious IDs\n",
    "        id_like = [c for c in df.columns\n",
    "                   if c.lower().endswith(\"id\") or c.lower().endswith(\"_id\") or c.lower()==\"id\"]\n",
    "        if id_like: df = df.drop(columns=id_like)\n",
    "\n",
    "        # X/y split (we synthesize features only; y unused)\n",
    "        if target is not None and target in df.columns:\n",
    "            X_raw = df.drop(columns=[target]).copy()\n",
    "        else:\n",
    "            X_raw = df.copy()\n",
    "\n",
    "        # Types\n",
    "        self.num_cols = X_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.cat_cols = [c for c in X_raw.columns if c not in self.num_cols]\n",
    "\n",
    "        # Rare-cap\n",
    "        if self.cat_cols:\n",
    "            cap_rare_inplace(X_raw, self.cat_cols, min_count=min_count, min_frac=min_frac)\n",
    "\n",
    "        # Pipelines\n",
    "        num_pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", MinMaxScaler(feature_range=(-1, 1))),\n",
    "        ])\n",
    "        try:\n",
    "            cat_pipe = Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, dtype=np.float32)),\n",
    "            ])\n",
    "        except TypeError:\n",
    "            cat_pipe = Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False, dtype=np.float32)),\n",
    "            ])\n",
    "\n",
    "        self.pre = ColumnTransformer(\n",
    "            transformers=[(\"num\", num_pipe, self.num_cols),\n",
    "                          (\"cat\", cat_pipe, self.cat_cols)],\n",
    "            remainder=\"drop\", verbose_feature_names_out=False\n",
    "        )\n",
    "\n",
    "        Xp = self.pre.fit_transform(X_raw).astype(np.float32)\n",
    "        self.X = torch.as_tensor(Xp, dtype=torch.float32, device=\"cpu\")\n",
    "\n",
    "        # One-hot group sizes (for inverse)\n",
    "        self.cat_group_sizes = []\n",
    "        if self.cat_cols:\n",
    "            oh: OneHotEncoder = self.pre.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "            for cats in oh.categories_:\n",
    "                self.cat_group_sizes.append(len(cats))\n",
    "\n",
    "    def _harden_onehots(self, X_fake_np: np.ndarray) -> np.ndarray:\n",
    "        if not self.cat_cols: return X_fake_np\n",
    "        out = X_fake_np.copy()\n",
    "        num_dim = len(self.num_cols)\n",
    "        start = num_dim\n",
    "        for g in self.cat_group_sizes:\n",
    "            if g <= 0: continue\n",
    "            block = out[:, start:start+g]\n",
    "            idx = np.argmax(block, axis=1)\n",
    "            block[:] = 0.0\n",
    "            block[np.arange(block.shape[0]), idx] = 1.0\n",
    "            out[:, start:start+g] = block\n",
    "            start += g\n",
    "        return out\n",
    "\n",
    "    def inverse_to_dataframe(self, X_fake: torch.Tensor) -> pd.DataFrame:\n",
    "        Xf = X_fake.detach().to(\"cpu\").numpy().astype(np.float32)\n",
    "        if self.num_cols:\n",
    "            num_dim = len(self.num_cols)\n",
    "            Xf[:, :num_dim] = np.clip(Xf[:, :num_dim], -1.0, 1.0)\n",
    "        Xf = self._harden_onehots(Xf)\n",
    "        try:\n",
    "            X_inv = self.pre.inverse_transform(Xf)\n",
    "            cols = self.num_cols + self.cat_cols\n",
    "            return pd.DataFrame(X_inv, columns=cols)\n",
    "        except Exception:\n",
    "            cols = [f\"f{i}\" for i in range(Xf.shape[1])]\n",
    "            return pd.DataFrame(Xf, columns=cols)\n",
    "\n",
    "# ------------------------\n",
    "# 3) Load data\n",
    "# ------------------------\n",
    "FILE_PATH = \"../data/Loan_default.csv\"   # adjust as needed\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "dataset = AutoTabularDataset(df, target=None, min_count=100, min_frac=0.01)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4) Move the WHOLE dataset to GPU once (big speedup)\n",
    "# ----------------------------------------------------\n",
    "X_device = dataset.X.to(DEVICE, non_blocking=True)  # one-time host→device copy\n",
    "N, in_dim = X_device.shape\n",
    "\n",
    "# ------------------------\n",
    "# 5) Models (balanced capacity)\n",
    "# ------------------------\n",
    "latent_dim = 64\n",
    "width_G, width_D = 1024, 768   # G slightly stronger than D (helps G learn)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z, d, w=width_G):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z, w), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(w, w), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(w, w), nn.LeakyReLU(0.2),  # 3 blocks total\n",
    "            nn.Linear(w, d), nn.Tanh(),\n",
    "        )\n",
    "    def forward(self, z): return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d, w=width_D):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, w), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(w, w), nn.LeakyReLU(0.2),  # 2–3 is enough; keep faster\n",
    "            nn.Linear(w, 1),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "G = Generator(latent_dim, in_dim).to(DEVICE)\n",
    "D = Discriminator(in_dim).to(DEVICE)\n",
    "\n",
    "# --------------------------------\n",
    "# 6) Loss, optimizers, EMA (quality)\n",
    "# --------------------------------\n",
    "criterion = nn.MSELoss()  # LSGAN: real≈1, fake≈0\n",
    "# Flip advantage: let G learn a tad faster than D\n",
    "optD = optim.Adam(D.parameters(), lr=2e-4, betas=(0.0, 0.99))\n",
    "optG = optim.Adam(G.parameters(), lr=3e-4, betas=(0.0, 0.99))\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
    "        self.keys = list(self.shadow.keys()); self.model = model\n",
    "    @torch.no_grad()\n",
    "    def update(self):\n",
    "        msd = self.model.state_dict()\n",
    "        for k in self.keys:\n",
    "            self.shadow[k].lerp_(msd[k].detach(), 1.0 - self.decay)\n",
    "    def copy_to(self, model: nn.Module): model.load_state_dict(self.shadow, strict=False)\n",
    "\n",
    "ema = EMA(G, decay=0.999)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 7) Choose MAXIMUM safe batch size (VRAM)\n",
    "# -----------------------------------------\n",
    "def find_max_batch(start=1024, limit=8192, step=2):\n",
    "    b = start\n",
    "    while b <= limit:\n",
    "        try:\n",
    "            b_eff = min(b, X_device.size(0))\n",
    "            tmp_real  = X_device[:b_eff]\n",
    "            tmp_ones  = torch.full((b_eff,1), 0.95, device=DEVICE)  # smoothed labels\n",
    "            tmp_zeros = torch.full((b_eff,1), 0.05, device=DEVICE)\n",
    "            tmp_z     = torch.randn(b_eff, latent_dim, device=DEVICE)\n",
    "            D.zero_grad(set_to_none=True); G.zero_grad(set_to_none=True)\n",
    "            loss = criterion(D(tmp_real), tmp_ones) + criterion(D(G(tmp_z).detach()), tmp_zeros)\n",
    "            loss.backward()  # probe memory use\n",
    "            b *= step\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                # clear best-effort; DirectML doesn't use CUDA cache but keep symmetry\n",
    "                break\n",
    "            else:\n",
    "                raise\n",
    "    return max(256, b // step)\n",
    "\n",
    "batch_size = find_max_batch(start=1024, limit=8192)\n",
    "# Preallocate big buffers (slice per step)\n",
    "ones_full  = torch.full((batch_size, 1), 0.95, device=DEVICE)\n",
    "zeros_full = torch.full((batch_size, 1), 0.05, device=DEVICE)\n",
    "z_buf      = torch.empty(batch_size, latent_dim, device=DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# 8) GPU-resident training loop\n",
    "# -----------------------------\n",
    "num_epochs = 20\n",
    "n_critic = 2               # 2 D steps per G step (on average) → more stable\n",
    "\n",
    "G.train(); D.train(); torch.set_grad_enabled(True)\n",
    "for epoch in range(num_epochs):\n",
    "    # Instance noise schedule (decays to 0)\n",
    "    sigma0 = 0.05\n",
    "    sigma  = max(0.0, sigma0 * (1 - epoch/num_epochs))\n",
    "\n",
    "    # shuffle indices ON GPU\n",
    "    perm = torch.randperm(N, device=DEVICE)\n",
    "    for i in range(0, N, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        real = X_device.index_select(0, idx)\n",
    "        bsz  = real.size(0)\n",
    "\n",
    "        real_lab = ones_full[:bsz]\n",
    "        fake_lab = zeros_full[:bsz]\n",
    "        z = z_buf[:bsz]; z.normal_()   # in-place random (no new alloc)\n",
    "\n",
    "        # --- D step(s): label smoothing + instance noise\n",
    "        optD.zero_grad(set_to_none=True)\n",
    "        real_noisy = real + (sigma * torch.randn_like(real) if sigma > 0 else 0.0)\n",
    "        fake = G(z)\n",
    "        fake_noisy = fake.detach() + (sigma * torch.randn_like(fake) if sigma > 0 else 0.0)\n",
    "\n",
    "        d_real = D(real_noisy);  loss_real = criterion(d_real, real_lab)\n",
    "        d_fake = D(fake_noisy);  loss_fake = criterion(d_fake, fake_lab)\n",
    "        (loss_real + loss_fake).backward()\n",
    "        optD.step()\n",
    "\n",
    "        # --- G step every n_critic\n",
    "        if ((i // batch_size) % n_critic) == 0:\n",
    "            optG.zero_grad(set_to_none=True)\n",
    "            g_loss = criterion(D(fake), real_lab)  # no noise, no detach\n",
    "            g_loss.backward()\n",
    "            optG.step()\n",
    "            ema.update()\n",
    "\n",
    "    # Epoch summary (kept light to avoid sync)\n",
    "    print(f\"Epoch {epoch+1:3d}: D_real={loss_real.item():.4f}  D_fake={loss_fake.item():.4f}  G={g_loss.item():.4f}  (bs={batch_size})\")\n",
    "\n",
    "# -------------------------------\n",
    "# 9) Sampling + inverse back to DF\n",
    "# -------------------------------\n",
    "G_eval = Generator(latent_dim, in_dim).to(DEVICE)\n",
    "ema.copy_to(G_eval); G_eval.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(1000, latent_dim, device=DEVICE)\n",
    "    X_fake = G_eval(z)\n",
    "\n",
    "synthetic_df = dataset.inverse_to_dataframe(X_fake)\n",
    "synthetic_df.head(100).to_csv(\"synthetic_preview.csv\", index=False)\n",
    "print(\"Saved synthetic_preview.csv (first 100 rows)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
